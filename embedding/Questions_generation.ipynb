{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iSJ63DvLrXz-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install langchain_huggingface langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"You are a senior recruiter with 10 years of experience. You always hire the best talents for companies.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "\n",
        "Generate 5 questions for the category {category} Topic: {topic}\n",
        "\n",
        "Please keep the questions varied and don't maintain the same meaning.\n",
        "\n",
        "Please conform with the following structure:\n",
        "- Question: question content\n",
        "- Difficulty: question difficulty. Easy/Medium/Hard\n",
        "- Category: question category\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R-9yjRYZrg-C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Google Gemini\n",
        "# model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature = 0.3)\n",
        "\n",
        "llm = \"llama-3.1-70b-versatile\"\n",
        "\n",
        "# Groq\n",
        "\n",
        "model = ChatGroq( model_name=llm, temperature = 0, groq_api_key = userdata.get('GROQ_API_KEY')\n",
        ")\n"
      ],
      "metadata": {
        "id": "OKRtd9KWrhC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_description = (\n",
        "    \"As a Data Scientist at 3D-Factory, you will be responsible for analyzing complex datasets to extract actionable insights. \"\n",
        "    \"Your role will involve developing and deploying predictive models and machine learning algorithms to tackle business challenges. \"\n",
        "    \"You will create clear visualizations and reports to effectively communicate findings to non-technical stakeholders, identify opportunities \"\n",
        "    \"for process improvements based on data, and collaborate with cross-functional teams to deliver data-driven solutions.\\n\\n\"\n",
        "    \"To succeed in this role, you should have a Bachelor’s or Master’s degree in Data Science, Computer Science, Statistics, Mathematics, \"\n",
        "    \"or a related field, with advanced degrees being a plus. You need to be proficient in programming languages such as Python or R and have \"\n",
        "    \"experience with machine learning and data analysis libraries like scikit-learn, TensorFlow, Keras, or PyTorch. Additionally, you should be \"\n",
        "    \"skilled in SQL and adept at managing both relational and non-relational databases. A solid knowledge of data visualization tools such as Tableau, \"\n",
        "    \"Power BI, or matplotlib is essential, along with strong analytical skills and expertise in statistical analysis and predictive modeling. The ability \"\n",
        "    \"to clearly communicate complex results to non-technical audiences is crucial. Previous experience as a Data Scientist or in a similar role is preferred, \"\n",
        "    \"with relevant internships or projects being appreciated. Experience with Big Data tools like Hadoop or Spark and familiarity with cloud environments \"\n",
        "    \"such as AWS, Azure, or Google Cloud are desirable.\"\n",
        ")\n",
        "prompt = \"\"\"Identify topics that can be used for interview questions based on this job description:\n",
        "\n",
        "{job_description}\n",
        "\n",
        "Please respect the following structure:\n",
        "[TOPIC_1, TOPIC_2, TOPIC_3]\"\"\""
      ],
      "metadata": {
        "id": "WV9XK2NHrhJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(input_variables=[\"job_description\"], template = prompt)\n",
        "chain = prompt | model\n",
        "\n",
        "# print(response.json())\n",
        "#topic = \"data science\"\n",
        "\n",
        "\n",
        "content = chain.invoke({'job_description': job_description }).content.strip()\n",
        "content"
      ],
      "metadata": {
        "id": "GWePzmONrhN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = [topic.strip() for topic in content[content.index('[')+1:content.index(']')].split(',')]"
      ],
      "metadata": {
        "id": "jns0El_LrhSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To-DO\n",
        "def generate_topics_from_job_description(job_description):\n",
        "  pass\n",
        "def generate_questions_from_topic(category, topic):\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "0O3jHR1FrhXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics"
      ],
      "metadata": {
        "id": "htQ88iC6riKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category = \"Data Science\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "\n",
        "Generate 5 questions for the category {category} Topic: {topic}\n",
        "\n",
        "Please keep the questions varied and don't maintain the same meaning.\n",
        "\n",
        "Please conform with the following structure:\n",
        "[question_content, difficulty_level]\n",
        "[question_content, difficulty_level]\n",
        "[question_content, difficulty_level]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "questions = []\n",
        "prompt_temp = PromptTemplate(input_variables=[\"category\", \"topic\"], template = prompt)\n",
        "chain = prompt_temp | model\n",
        "\n",
        "for topic in topics:\n",
        "\n",
        "  # GEN\n",
        "\n",
        "  questions_raw = chain.invoke({'topic': topic, 'category': category }).content.strip()\n",
        "  # type questions_raw? str\n",
        "  # split\n",
        "  loc_questions = [question[question.index('[')+1: question.index(']')].replace('\"', '').split(',') for question in questions_raw.split('\\n')]\n",
        "\n",
        "  questions += loc_questions\n",
        "\n",
        "  # structure is not stable"
      ],
      "metadata": {
        "id": "sDyQ699sriRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions"
      ],
      "metadata": {
        "id": "0bOE1I95ric9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(questions)"
      ],
      "metadata": {
        "id": "5MCchUWZrilP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "loc_questions = [question[question.index('[')+1: question.index(']')].replace('\"', '').split(',') for question in questions_raw.split('\\n')]"
      ],
      "metadata": {
        "id": "nhWlVTAKskR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install flash_attn==2.5.8 torch==2.3.1 accelerate==0.31.0 transformers==4.43.0"
      ],
      "metadata": {
        "id": "GdLiAh9uslvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Source\n",
        "#\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "model_id = \"microsoft/phi-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256\n",
        ")\n",
        "\n",
        "hf = HuggingFacePipeline(pipeline=pipe)\n"
      ],
      "metadata": {
        "id": "jeMi37pMsmCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.invoke(prompt) # cpu"
      ],
      "metadata": {
        "id": "H6TiVaBosuci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.invoke(f'You are a recruiter. You will be given a test to a job applicant. Question for an interview about the topic {topic}: The first question is')"
      ],
      "metadata": {
        "id": "6_c2KPwRsur3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "\n",
        "cuda.empty_cache()"
      ],
      "metadata": {
        "id": "y9nZT7CgsvOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install accelerate bitsandbytes xformers adjustText transformers"
      ],
      "metadata": {
        "id": "xNcS_7Udsvtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # 4-bit quantization\n",
        "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
        "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
        "    bnb_4bit_compute_dtype=bfloat16  # Computation types\n",
        ")\n",
        "\n",
        "# Llama 2 Tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, token = hf_token)\n",
        "\n",
        "# Llama 2 Model\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    token = hf_token,\n",
        "  )\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Our text generator\n",
        "generator = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=500,\n",
        "    repetition_penalty=1.1 #\n",
        ")\n",
        "\n",
        "prompt = f\"Generate an interview question for the topic: {topic}\"\n",
        "res = generator(prompt)\n",
        "res[0]['generated_text']"
      ],
      "metadata": {
        "id": "czdI42PNs5c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-ku3J8-s5rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdaxmFy-s5-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZ8EPeyXs6n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vkUmMt94s7TO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}